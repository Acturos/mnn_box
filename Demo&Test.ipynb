{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The demo and test note of MNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnn_pytorch import *\n",
    "import numpy as np\n",
    "import mnnbox as mnn\n",
    "import torch\n",
    "import time\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the key parameters for running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES = 100000\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 100000\n",
    "NEURONS = 10\n",
    "HIDDEN = 1\n",
    "\n",
    "input_mean = np.random.uniform(1.9, 2.2, size=(TOTAL_SAMPLES, NEURONS))\n",
    "input_std = np.random.uniform(9.8, 10.2, size=(TOTAL_SAMPLES, NEURONS))\n",
    "target_mean = np.random.uniform(1.9, 2.2, size=(TOTAL_SAMPLES, NEURONS))\n",
    "target_std = np.random.uniform(9.8, 10.2, size=(TOTAL_SAMPLES, NEURONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnn_x = mnn.Input(value = np.empty((TOTAL_SAMPLES, NEURONS, 2)), name=\"input\")\n",
    "mnn_target = mnn.Variable(value=np.empty((TOTAL_SAMPLES, NEURONS, 2)), name=\"target\")\n",
    "mnn_all_weight = mnn.Variable(value=mnn.truncated_normal([NEURONS, NEURONS], stddev=0.1), name=\"weight\")\n",
    "\n",
    "mnn_x.value = np.stack([input_mean, input_std], axis=-1)\n",
    "mnn_target.value = np.stack([target_mean, target_std], axis=-1)\n",
    "mnn_activate = mnn.Activate(mnn_x)\n",
    "cost = mnn.MSE(mnn_activate,mnn_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a simplest Mnn: Input -> Activate -> Out -> Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The activation function of mnn_box cost: 50.70154809951782\n",
      "The activation function of mnn_box cost: 0.8059999942779541\n",
      "The time cost of one pass is: 51.58254790306091\n"
     ]
    }
   ],
   "source": [
    "mnn_x.forward()\n",
    "start_time1 = time.time()\n",
    "mnn_activate.forward()\n",
    "mnn_forward_time = time.time() - start_time1\n",
    "print(\"The activation function of mnn_box cost:\", mnn_forward_time)\n",
    "cost.forward()\n",
    "cost.backward()\n",
    "start_time2 = time.time()\n",
    "mnn_activate.backward()\n",
    "mnn_backward_time = time.time() - start_time2\n",
    "print(\"The activation function of mnn_box cost:\", mnn_backward_time)\n",
    "mnn_x.backward()\n",
    "mnn_total = time.time() - start_time1\n",
    "print(\"The time cost of one pass is:\", mnn_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the pytorch version to compare the accuracy and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_mean_in = torch.from_numpy(input_mean)\n",
    "py_std_in = torch.from_numpy(input_std)\n",
    "py_mean_target = torch.from_numpy(target_mean)\n",
    "py_std_target = torch.from_numpy(target_std)\n",
    "\n",
    "py_mean_in.requires_grad = True\n",
    "py_std_in.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(py_mean_in.grad)\n",
    "print(py_std_in.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The forward activation of pytorch version cost: 2.209994316101074\n",
      "The backward activation of pytorch version cost: 2.366999626159668\n",
      "Total time cost of one pass:  4.576993942260742\n"
     ]
    }
   ],
   "source": [
    "start_time3 = time.time()\n",
    "activated_mean = Mnn_Activate_Mean.apply(py_mean_in, py_std_in)\n",
    "activated_std = Mnn_Activate_Std.apply(py_mean_in, py_std_in)\n",
    "py_forward_time = time.time() - start_time3\n",
    "print(\"The forward activation of pytorch version cost:\", py_forward_time)\n",
    "py_mean_in.retain_grad()\n",
    "py_std_in.retain_grad()\n",
    "loss = loss_function(activated_mean, activated_std, py_mean_target, py_std_target)\n",
    "loss.backward()\n",
    "py_total = time.time() - start_time3\n",
    "py_backward_time = py_total - py_forward_time\n",
    "print(\"The backward activation of pytorch version cost:\", py_backward_time)\n",
    "print(\"Total time cost of one pass: \", py_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_loss = loss.clone().detach().numpy() - cost.value\n",
    "py_mean_out = activated_mean.clone().detach().numpy().flatten()\n",
    "py_std_out = activated_std.clone().detach().numpy().flatten()\n",
    "py_mean_grad = py_mean_in.grad.clone().detach().numpy().flatten()\n",
    "py_std_grad = py_std_in.grad.clone().detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnn_mean_out = mnn_activate.value[:,:,0].flatten()\n",
    "mnn_std_out = mnn_activate.value[:,:,1].flatten()\n",
    "mnn_mean_grad = list(mnn_activate.gradients.values())[0][:,:,0].flatten()\n",
    "mnn_std_grad = list(mnn_activate.gradients.values())[0][:,:,1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_mean = np.abs(py_mean_out - mnn_mean_out)\n",
    "comp_std = np.abs(py_std_out - mnn_std_out)\n",
    "comp_mean_percent = comp_mean / np.abs(mnn_mean_out) * 100\n",
    "comp_std_percent = comp_std / np.abs(mnn_mean_out) * 100\n",
    "                   \n",
    "comp_mean_grad = np.abs(py_mean_grad - mnn_mean_grad)\n",
    "comp_std_grad = np.abs(py_std_grad - mnn_std_grad)\n",
    "comp_mean_grad_percent = comp_mean_grad / np.abs(mnn_mean_grad) * 100\n",
    "comp_std_grad_percent = comp_std_grad / np.abs(mnn_std_grad) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0006371014258519381\n",
      "4.361351624543319e-06 1.6230370322267925e-06 0.0034923480239515523 0.0012966999754939483\n",
      "9.290188340738137e-05 3.71452568011119e-05 0.0743076041706477 0.029782921598036596\n",
      "1.946102609138009e-09 1.5494510520514364e-09 0.46847957987365296 0.378407598558566\n",
      "3.7259284681549974e-10 2.688457541055225e-10 0.2783340881207034 0.2163090677224089\n"
     ]
    }
   ],
   "source": [
    "print(comp_loss)\n",
    "print(np.max(comp_mean), np.mean(comp_mean), np.max(comp_mean_percent), np.mean(comp_mean_percent))\n",
    "print(np.max(comp_std), np.mean(comp_std), np.max(comp_std_percent), np.mean(comp_std_percent))\n",
    "print(np.max(comp_mean_grad), np.mean(comp_mean_grad), np.max(comp_mean_grad_percent), np.mean(comp_mean_grad_percent))\n",
    "print(np.max(comp_std_grad), np.mean(comp_std_grad), np.max(comp_std_grad_percent), np.mean(comp_std_grad_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch版 与 原 Mnn 性能比较\n",
    "#### 初始参数\n",
    "* 测试模型： input -> activation -> output -> MSE Loss\n",
    "* Total samples: 100000\n",
    "* Neurons : 10\n",
    "* Mean: [1.9, 2.2] 均匀分布 \n",
    "* std : [9.8, 10.2] 均匀分布\n",
    "\n",
    "即进行了2\\*10\\*100000 次激活计算\n",
    "#### 运行时间\n",
    "##### 原MNN\n",
    "* forward： 50.70s\n",
    "* backward:  0.80s\n",
    "* total : 51.58s\n",
    "\n",
    "##### Pytorch版\n",
    "* forward: 2.21s\n",
    "* backward: 2.37s\n",
    "* total: 4.57s\n",
    "\n",
    "pytorch版速度有极大提升，但由于pytorch框架限制，backward不方便利用之前的计算结果，故需要重新计算拖累运行效率，后续再考虑优化问题。\n",
    "\n",
    "#### 计算误差比较：\n",
    "* MSE loss ： 相差0.0006\n",
    "* Mean Output: 最大误差 0.0035\\%，平均 0.0013\\%\n",
    "* Std Output: 最大误差 0.074\\%，平均 0.03\\%\n",
    "* Mean Backward gradient: 最大误差 0.47\\%，平均0.38\\%\n",
    "* Std Backward gradient: 最大误差 0.28\\%， 平均0.21\\%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
